{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to TF-IDF and Doc Clustering\n",
    "\n",
    "### Lynn Cherny, arnicas@gmail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# You can ignore the pink warning that appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.data.path = ['../nltk_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency, Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Term Frequency**: Number of appearances of a word in a document (the counts we saw already)\n",
    "\n",
    "**Document Frequency**: Number of documents that contain a word in a set of docs\n",
    "\n",
    "**TF-IDF** is **Term Frequency / Document Frequency**, with some extra fiddles.\n",
    "\n",
    "Example from [Manning, Raghavan, and Schuetze](http://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html) showing IDF of a rare term is high:\n",
    "\n",
    "\n",
    "<img src=\"img/doc_freq.png\">\n",
    "\n",
    "\n",
    "TF-IDF for a word and document is usually calculated as:\n",
    "\n",
    "**(Word t's frequency in the doc) * Log( Number of Docs / Number of docs that contain the word t)**\n",
    "\n",
    "However, it is usually done with a + 1 term or two.  You can consider it an information measure for document words (or \"features\") in a bag-of-words style analysis, where the order of the words doesn't matter, just the set of words. It is a **\"weight\"** for a word. Some features of TF-IDF:\n",
    "\n",
    "* If a term is very frequent in the whole doc set, it's less interesting overall and gets a low TF-IDF. Note this tends to remove stopwords for you! However, you need a lot of documents for this to work well. Beware of effects of tf-idf on small doc sets, it may not work as you expect.\n",
    "* A term frequent in a few docs, but not in a lot, has a high score. It helps distinguish those docs.\n",
    "\n",
    "See the discussion in [Manning, Raghavan, and Schuetze](http://nlp.stanford.edu/IR-book/html/htmledition/term-frequency-and-weighting-1.html), and even [more math in Wikipedia](http://en.wikipedia.org/wiki/Tf%E2%80%93idf). Depending on implementation, TF-IDF may or may not be normalized. **Always check to see if the implementation you use cleans stopwords or not and decide if you like that.**\n",
    "\n",
    "Some more python references:\n",
    "* [Demo using TextBlob, another lib](http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/)\n",
    "* [A version written on top of NLTK](https://github.com/yebrahim/TF-IDF-Generator)\n",
    "* [TF-IDF in gensim](http://radimrehurek.com/gensim/tutorial.html)\n",
    "* [TF-IDF in scikit-learn](http://scikit-learn.org/stable/modules/feature_extraction.html)\n",
    "\n",
    "In other languages than Python, for instance:\n",
    "* [A version in Processing by Nic Felton](https://github.com/feltron/Processing_TFIDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code example from Building Machine Learning Systems with Python (Richert & Coelho) \n",
    "# - modified slightly by Lynn\n",
    "\n",
    "import math\n",
    "\n",
    "def tfidf(t, d, D):\n",
    "    tf = float(d.count(t)) / sum(d.count(w) for w in set(d))  # normalized\n",
    "    # Note his version doesn't use +1 in denominator.\n",
    "    idf = math.log( float(len(D)) / (len([doc for doc in D if t in doc])))\n",
    "    return tf * idf\n",
    "\n",
    "\n",
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]  # try adding another c to the last doc!\n",
    "D = [a, abb, abc]\n",
    "\n",
    "print(tfidf(\"a\", a, D))   # a is in all of them\n",
    "print(tfidf(\"a\", abc, D)) # a is in all of them\n",
    "print(tfidf(\"b\", abc, D)) # b occurs only once here, but in 2 docs\n",
    "print(tfidf(\"b\", abb, D)) # b occurs more frequently in this doc\n",
    "print(tfidf(\"c\", abc, D)) # c is unique in the doc set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What if you change some of those docs, or add another one? Add another c in the last doc, e.g.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filelist = !ls ../data/movie_reviews/positive/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Utilities to Make a File We Can Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.word('english')\n",
    "\n",
    "import collections\n",
    "def clean_tokens(tokens, stopwords):\n",
    "    import string\n",
    "    \"\"\" Lowercases, takes out punct and stopwords and short strings \"\"\"\n",
    "    return [token.lower() for token in tokens if (token not in string.punctuation)and (token.lower() not in stopwords) and len(token) > 2]\n",
    "\n",
    "def makeText(filename, stopwords):\n",
    "    from nltk import Text\n",
    "    with open(filename) as handle:\n",
    "        text = handle.read()\n",
    "    return Text(clean_tokens(nltk.word_tokenize(text.decode('ascii', 'ignore')), stopwords))\n",
    "\n",
    "def makeTextCollection(files, stopwords=stopwords):\n",
    "    from nltk import TextCollection\n",
    "    texts= [makeText(filename, stopwords) for filename in files]\n",
    "    collection = TextCollection(texts)\n",
    "    return collection, texts\n",
    "\n",
    "# use the data for the vocab in a single doc for a wordcloud, for instance\n",
    "def compute_tfidf_by_doc(coll, texts, filenames):\n",
    "    tfidf_by_doc = collections.defaultdict(list)\n",
    "    for i, text in enumerate(texts):\n",
    "        for word in set(text.tokens):   # just use the words in this text\n",
    "            tfidfscore = coll.tf_idf(word, text)\n",
    "            tf = coll.tf(word, text) # is actually count / len(text)\n",
    "            count = text.count(word)\n",
    "            if tfidfscore:\n",
    "                tfidf_by_doc[filenames[i]].append({\n",
    "                    \"word\": word,\n",
    "                    \"tfidf\": tfidfscore,\n",
    "                    \"tf\": tf,\n",
    "                    \"count\": count\n",
    "                })\n",
    "    return tfidf_by_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need to make the text collection, then use it to compute the tf-idf for the words in the docs.\n",
    "res = makeTextCollection(filelist)\n",
    "coll = res[0]\n",
    "texts = res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coll.tf_idf(\"woman\", texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfs = compute_tfidf_by_doc(coll, texts, filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfs[tfidfs.keys()[0]]  # the first filename is the first key... it contains a list of words and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "jsonified = json.dumps(tfidfs)\n",
    "with open('../outputdata/pos_movies_tfidf.json', 'w') as handle:\n",
    "    handle.write(jsonified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -al ../outputdata/pos_movies_tfidf.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at these reviews as little wordclouds, using different measures to size our words. Let's work with **word_clouds_tfidf.html** and we can compare how our clouds look using regular word counts, term frequencies (which is count / length of the document), and tfidf across all the documents.\n",
    "\n",
    "For movie cv681_tok-28559.txt, by counts is useless:\n",
    "\n",
    "<img src=\"img/counts.png\">\n",
    "\n",
    "By tf (term frequency, normalized to document length), it's a bit better, in that some of the smaller words are getting larger:\n",
    "\n",
    "<img src=\"img/tf.png\">\n",
    "\n",
    "It's still not stellar with tf-idf, but the more unique words are popping a bit better and the words shared across many docs, like \"film\" and \"movie\" are disappearing:\n",
    "\n",
    "<img src=\"img/tfidf.png\">\n",
    "\n",
    "TF-IDF has started to creep into journalism now.  Here are a couple of articles that use it:\n",
    "\n",
    "* http://fivethirtyeight.com/features/these-are-the-phrases-that-sanders-and-clinton-repeat-most/\n",
    "* http://blog.chartbeat.com/2015/10/22/identifying-and-clustering-news-events-by-content/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to discuss vectors, and similarity measures!\n",
    "\n",
    "This is where some more libraries start to be needed. NLTK is fine for some things, but can be very slow for stuff like making vectors of terms across large numbers of documents.\n",
    "\n",
    "Each document is a collection of weighted words, which we'll call a vector.  Vectors can be compared to each other, to compute similarity. A common metric is \"cosine similarity.\" Image from \n",
    "Manning, Raghavan and Schuetze:\n",
    "\n",
    "<img src=\"img/cosine_similarity.png\">\n",
    "\n",
    "Reminder: Angles close to each other are near 1 in cosine, far apart are closer to 0. This means that in practice you may want to subtract from 1, so that a higher score = further away. You should think of it as cosine = distance, 1-cos = similarity.  Pattern (the library) does this for you so that similarity = 1 - cos.\n",
    "\n",
    "Another, perhaps simpler to understand, is euclidean distance (image from [this article](https://de.dariah.eu/tatom/working_with_text.html)):\n",
    "\n",
    "<img src=\"img/euclidean.png\">\n",
    "\n",
    "This is essentially the hypoteneuse between two sides of a vector triangle. Larger numbers = further apart vectors!\n",
    "\n",
    "Links:\n",
    "* Reference in Manning, Raghavan and Schuetze: http://nlp.stanford.edu/IR-book/html/htmledition/dot-products-1.html\n",
    "* A good article focusing on queries in search -- same idea! https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/\n",
    "* Computing the angle between vectors for cosine similarity: http://www.mathsisfun.com/algebra/vectors-dot-product.html\n",
    "* An article on text similarities using scikit-learn: https://de.dariah.eu/tatom/working_with_text.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the docs... again.  We're going to make TF-IDF vectors with sklearn (scikit-learn) because it's faster.\n",
    "\n",
    "def load_texts(filenames, dirpath):\n",
    "    \"\"\" filenames are the leaves, dirpath is the path to them with the / \"\"\"\n",
    "    loaded_text = {}\n",
    "    for filen in filenames:\n",
    "        with open(dirpath + filen) as handle:\n",
    "            loaded_text[filen] = handle.read()\n",
    "    return loaded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = load_texts(filelist, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer().fit_transform([text.decode('ascii', 'ignore') for text in texts.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If you haven't managed to load sklearn, you can try the slower version of the vectorizing:\n",
    "def get_unique_terms_for_all(collection):\n",
    "    #get a list of unique terms\n",
    "    unique_terms = list(set(collection))\n",
    "    print \"Unique terms found: \", len(unique_terms)\n",
    "    return unique_terms\n",
    "\n",
    "# Function to create a TF*IDF vector for one document.  For each of\n",
    "# our unique words, we have a feature which is the td*idf for that word\n",
    "# in the current document\n",
    "def TFIDF(document, unique_terms_in_all):\n",
    "    word_tfidf = []\n",
    "    for word in unique_terms_in_all:\n",
    "        word_tfidf.append(collection.tf_idf(word,document))\n",
    "    return word_tfidf\n",
    "\n",
    "def tfidf_doc_vectors(texts, coll):\n",
    "    uniques = get_unique_terms_for_all(coll)\n",
    "    vectors = [numpy.array(TFIDF(f, uniques)) for f in texts]\n",
    "    print \"Vectors created.\"\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us the input we need for clustering and making dendrograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing the Cluster Tree with SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scipy's pdist is pairwise distance - see http://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
    "You can use cosine here as well! or a host of other options...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist = pdist(vectors, metric='cosine')  # look at the manpage and pick a different measure to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linkage(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is a base diagram, using defaults... \n",
    "dendrogram(linkage(dist))  # this plotting function has a ton of things you can manipulate if you look at the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts[texts.keys()[14]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do this with a nicer layout now... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_dend(data, labels=None, height=6):\n",
    "    from pylab import rcParams\n",
    "    dist = pdist(data, metric='cosine')\n",
    "    link = linkage(dist, method='complete')\n",
    "    rcParams['figure.figsize'] = 6, height\n",
    "    rcParams['axes.labelsize'] = 5\n",
    "    if not labels:\n",
    "        dend = dendrogram(link, orientation='right') #labels=names)\n",
    "    else:\n",
    "        dend = dendrogram(link, orientation='right', labels=[str(i) + label for i, label in enumerate(labels)])\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist = make_dend(vectors, height=15, labels=texts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's inspect a pair that are grouped closely in the cosine-similarity tree -- 23, 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts.keys()[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts[texts.keys()[23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts[texts.keys()[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What do you notice about them both?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### This is an example of a heatmap based on similarity scores.  Most of these are not very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code borrowed from: http://nbviewer.ipython.org/github/OxanaSachenkova/hclust-python/blob/master/hclust.ipynb\n",
    "\n",
    "def make_heatmap_matrix(dist, method='complete'):\n",
    "    \"\"\" Pass in the distance matrix; method options are complete or single \"\"\"\n",
    "    # Compute and plot first dendrogram.\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    # x ywidth height\n",
    "    ax1 = fig.add_axes([0.05,0.1,0.2,0.6])\n",
    "    Y = linkage(dist, method=method)\n",
    "    Z1 = dendrogram(Y, orientation='right') # adding/removing the axes\n",
    "    ax1.set_xticks([])\n",
    "\n",
    "    # Compute and plot second dendrogram.\n",
    "    ax2 = fig.add_axes([0.3,0.71,0.6,0.2])\n",
    "    Z2 = dendrogram(Y)\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "\n",
    "    #Compute and plot the heatmap\n",
    "    axmatrix = fig.add_axes([0.3,0.1,0.6,0.6])\n",
    "    idx1 = Z1['leaves']\n",
    "    idx2 = Z2['leaves']\n",
    "    D = squareform(dist)\n",
    "    D = D[idx1,:]\n",
    "    D = D[:,idx2]\n",
    "    im = axmatrix.matshow(D, aspect='auto', origin='lower', cmap=plt.cm.YlGnBu)\n",
    "    axmatrix.set_xticks([])\n",
    "    axmatrix.set_yticks([])\n",
    "\n",
    "    # Plot colorbar.\n",
    "    axcolor = fig.add_axes([0.91,0.1,0.02,0.6])\n",
    "    plt.colorbar(im, cax=axcolor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_heatmap_matrix(dist, method='complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant links:\n",
    "* http://stackoverflow.com/questions/20176590/plot-the-centroid-values-over-the-existing-plot-using-matplotlib\n",
    "* http://glowingpython.blogspot.jp/2012/04/k-means-clustering-with-scipy.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering with NLTK Using Our TFIDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## clustering in NLTK:\n",
    "import numpy\n",
    "from nltk.cluster import KMeansClusterer, GAAClusterer, euclidean_distance\n",
    "import nltk.corpus\n",
    "import nltk.stem\n",
    "stemmer_func = nltk.stem.snowball.SnowballStemmer(\"english\").stem\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "cluster = KMeansClusterer(4, euclidean_distance)\n",
    "cluster.cluster(vectors)\n",
    "classified_examples = [cluster.classify(vec) for vec in vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,val in enumerate(classified_examples):\n",
    "    print val, texts.keys()[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the items in cluster 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts['../data/movie_reviews/positive/cv677_tok-11867.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts['../data/movie_reviews/positive/cv684_tok-10367.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts['../data/movie_reviews/positive/cv683_tok-12295.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For another, more advanced tour of clustering, including plotting these, see the notebook cluster_analysis_by_brandon_rose.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
